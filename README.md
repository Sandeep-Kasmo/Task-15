ğŸ“„ README.md â€” Resume Parsing ETL Pipeline
ğŸš€ Project Overview

This project automates the end-to-end pipeline of resume extraction, parsing, transformation, and loading into a MySQL database.

The workflow is:

Extract

Fetch resume PDF from AWS S3

Convert PDF to raw text â†’ temp_resume_text.txt

Transform

Parse text using custom regex-based logic

Extract:

Name

Email

Summary

Technical Skills

Internship Experience (numbered blocks supported)

Construct a clean single-row Pandas DataFrame

Load

Create MySQL table dynamically based on DataFrame structure

Convert lists/dicts â†’ JSON strings

Insert parsed data into MySQL

Handle NULLs, datatypes, long text, etc.

ğŸ—‚ï¸ Project Structure

Resume_Parsing/

â”‚

â”œâ”€â”€ src/

â”‚   â”œâ”€â”€ extract.py          # Handles S3 download and PDF â†’ text extraction

â”‚   â”œâ”€â”€ transform.py        # Contains resume text parsing functions

â”‚   â”œâ”€â”€ load.py             # MySQL table creation + data insertion logic

â”‚   â”œâ”€â”€ trf.py              # Main driver script (ETL pipeline)

â”‚   â”œâ”€â”€ temp_resume_text.txt# Generated raw text from resume

â”‚

â”œâ”€â”€ README.md

â””â”€â”€ requirements.txt

ğŸ“¥ 1. Extraction Module (extract.py)

Responsibilities:

Connect to AWS S3

Download resume PDF

Extract text using PyPDF2

Save to local file:

temp_resume_text.txt


Key Function:

raw_text = extract_and_save_text(bucket_name, source_key, LOCAL_TEXT_FILE)


If the extraction succeeds:

temp_resume_text.txt created successfully

ğŸ§  2. Transformation Module (transform.py)

This module reads the raw text and extracts structured fields.

âœ” Extracted Fields

Name â€” first non-empty line

Email â€” regex-based

Summary â€” extracted between OBJECTIVE and next section

Skills â€” matched from predefined keyword list

Experience_List â€” extracted from numbered internship blocks

âœ” Internship Parsing Logic

Supports:

1. Internship Title (XYZ)
   Description...
2) Another Internship
- Or bullet-style entries


First non-empty line from each block is extracted, cleaned, and converted into:

"AICTE Internship ; Cognizant Agile Internship ; ..."

âœ” Output

A single-row Pandas DataFrame:

df = parse_resume_text(raw_text)

ğŸ—ƒï¸ 3. Load Module (load.py)

This module:

Establishes MySQL connection

Creates a table dynamically based on DataFrame columns

Converts:

lists â†’ JSON strings

NaN â†’ NULL

timestamp â†’ datetime

âœ” Example Table Definition
CREATE TABLE parsed_resume (
    Name TEXT,
    Email TEXT,
    Summary TEXT,
    Skills TEXT,
    Experience_List TEXT
);

âœ” Insertion Logic
load_data(df, "parsed_resume")


Outputs:

Inserted 1 rows into parsed_resume
Table parsed_resume created successfully

ğŸ§ª 4. Running the Full ETL Pipeline

Run:

python trf.py


The script performs:

Download from S3

Extract text

Parse into DataFrame

Load into MySQL

ğŸ”§ Configuration
ğŸ“ AWS S3

Set bucket and key in extract.py:

S3_BUCKET_NAME = "your_bucket"
S3_SOURCE_KEY = "incoming/resume.pdf"

ğŸ“ MySQL Credentials

Edit in load.py:

conn = mysql.connector.connect(
    host='localhost',
    user='root',
    password='',
    database='PythonLearningDB'
)

ğŸ“¦ Installation

Install dependencies:

pip install pandas boto3 PyPDF2 mysql-connector-python

ğŸš§ Limitations

The resume parser is simple and regex-based.

Different resume formats require adjustments.

The skills list must be expanded manually.

ğŸ™Œ Future Improvements

Add ML-based parser like spaCy / transformers

Add section-level sentiment / similarity scoring

Support for DOCX resumes

Auto-detection of resume structure

Deploy pipeline using AWS Lambda + RDS

ğŸ“§ Contact

For improvements or help, reach out anytime.
